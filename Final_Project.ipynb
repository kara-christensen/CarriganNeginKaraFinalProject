{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2dsN0NdysAB"
   },
   "outputs": [],
   "source": [
    "# Import the libraries we'll use below.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Might need to install SMOTE. After install, I needed to close anaconda/jupyter notebook and reopen it for it to\n",
    "## work :) - Kara\n",
    "##Also import opencv-python for image augmentation- Negin\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-wvLSpkzGY5"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('KidneyImages/kidneyData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "ydav9VmY0JNs",
    "outputId": "5083a436-fa8f-48ac-eb07-6e0afd72c5cd"
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsize = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0h3ndqZEsXT"
   },
   "outputs": [],
   "source": [
    "## Read in all images\n",
    "# First, read in all cyst images\n",
    "\n",
    "cystImages = []\n",
    "\n",
    "# Get the correct folder of images\n",
    "folder_dir = \"KidneyImages/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Cyst\"\n",
    "\n",
    "\n",
    "for images in os.listdir(folder_dir): # For each item in the folder\n",
    "    if (images.endswith(\".jpg\")): # If it is a jpg\n",
    "        image = Image.open(folder_dir + '/' + images) # Open the image\n",
    "        image = image.resize(newsize) # Resize it to 256x256\n",
    "        npImage = np.asarray(image) # Turn it into an array\n",
    "        reshapedImage = npImage[:, :, 0] # Get rid of the last two color values (since it is black and white)\n",
    "        cystImages.append({'image_id': images[:-4], 'image': reshapedImage}) # Add the information to the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nz5nTPfA8XzJ"
   },
   "outputs": [],
   "source": [
    "# Then all normal images\n",
    "normalImages = []\n",
    "\n",
    "folder_dir = \"KidneyImages/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Normal\"\n",
    "\n",
    "\n",
    "for images in os.listdir(folder_dir):\n",
    "    if (images.endswith(\".jpg\")):\n",
    "        image = Image.open(folder_dir + '/' + images)\n",
    "        image = image.resize(newsize)\n",
    "        npImage = np.asarray(image)\n",
    "        reshapedImage = npImage[:, :, 0]\n",
    "\n",
    "        normalImages.append({'image_id': images[:-4], 'image': reshapedImage})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nynum1gE84tY"
   },
   "outputs": [],
   "source": [
    "# Then all stone images\n",
    "stoneImages = []\n",
    "\n",
    "folder_dir = \"KidneyImages/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Stone\"\n",
    "\n",
    "\n",
    "for images in os.listdir(folder_dir):\n",
    "    if (images.endswith(\".jpg\")):\n",
    "        image = Image.open(folder_dir + '/' + images)\n",
    "        image = image.resize(newsize)\n",
    "        npImage = np.asarray(image)\n",
    "        reshapedImage = npImage[:, :, 0]\n",
    "\n",
    "        stoneImages.append({'image_id': images[:-4], 'image': reshapedImage})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_nlQuIx-J-e"
   },
   "outputs": [],
   "source": [
    "# Then all tumor images\n",
    "tumorImages = []\n",
    "\n",
    "folder_dir = \"KidneyImages/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/CT-KIDNEY-DATASET-Normal-Cyst-Tumor-Stone/Tumor\"\n",
    "\n",
    "\n",
    "for images in os.listdir(folder_dir):\n",
    "    if (images.endswith(\".jpg\")):\n",
    "        image = Image.open(folder_dir + '/' + images)\n",
    "        image = image.resize(newsize)\n",
    "        npImage = np.asarray(image)\n",
    "        reshapedImage = npImage[:, :, 0]\n",
    "\n",
    "        tumorImages.append({'image_id': images[:-4], 'image': reshapedImage})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrW3hRCtizO3"
   },
   "outputs": [],
   "source": [
    "# Concatenate all list of images, and turn into a data frame\n",
    "\n",
    "allImages = cystImages + stoneImages + normalImages + tumorImages\n",
    "\n",
    "imgs = pd.DataFrame(allImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary of labels\n",
    "labelDictionary = {0: 'Cyst', 1: 'Normal', 2: 'Stone', 3:'Tumor'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the other dataframe to get each image matched with its label\n",
    "imgWithLabel = imgs.merge(df, how = 'inner', on = 'image_id')\n",
    "imgWithLabel = imgWithLabel[['image', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imgWithLabel['image'].to_numpy()\n",
    "y = imgWithLabel['target'].to_numpy()\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape/ unpack the array of images, then flatten it so it is prepared for smote\n",
    "X = np.array([x for x in X])\n",
    "print(X.shape)\n",
    "nsamples, nx, ny = X.shape\n",
    "X = X.reshape((nsamples,nx*ny))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE to balance classes\n",
    "\n",
    "# Original class distribution\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "uniqueLables = [labelDictionary[x] for x in unique]\n",
    "plt.pie(counts, labels = uniqueLables)\n",
    "plt.title('Original Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Perform SMOTE sampling\n",
    "oversample = SMOTE()\n",
    "Xsmote, ysmote = oversample.fit_resample(X, y)\n",
    "\n",
    "\n",
    "# Updated class distribution\n",
    "uniquesmote, countssmote = np.unique(ysmote, return_counts=True)\n",
    "uniqueLablessmote = [labelDictionary[x] for x in uniquesmote]\n",
    "plt.pie(countssmote, labels = uniqueLablessmote)\n",
    "plt.title('Post Smote Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle & reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "indices = np.arange(Xsmote.shape[0])\n",
    "shuffled_indices = np.random.permutation(indices)\n",
    "Xshuffled = Xsmote[shuffled_indices]\n",
    "yshuffled = ysmote[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to 4 dimensional data for augmentation\n",
    "Xshuffled = Xshuffled.reshape((20308,256,256,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = plt.imshow(Xshuffled[1,:,:,:],cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Xshuffled, yshuffled, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an intermediary validation set \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create augmentation layers\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomRotation(0.1),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = X_train[2]\n",
    "print(example_image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rotate_image(image):\n",
    "#     # Random angle between -20 and 20 degrees\n",
    "#     angle = np.random.uniform(-20, 20)\n",
    "#     rotated_image = np.interp(np.arange(len(image)), np.arange(len(image)) + angle, image, left=0, right=0)\n",
    "#     return rotated_image\n",
    "\n",
    "# def zoom_image(image):\n",
    "#     # Random zoom factor between 0.8 and 1.2\n",
    "#     zoom_factor = np.random.uniform(0.8, 1.2)\n",
    "#     zoomed_image = np.interp(np.arange(len(image)), np.arange(len(image)) * zoom_factor, image, left=0, right=0)\n",
    "#     return zoomed_image\n",
    "\n",
    "# def flip_image(image):\n",
    "#     # Randomly flip the image horizontally\n",
    "#     if np.random.rand() > 0.5:\n",
    "#         flipped_image = np.flip(image)\n",
    "#     else:\n",
    "#         flipped_image = image\n",
    "#     return flipped_image\n",
    "\n",
    "# def augment_image(image):\n",
    "#     # Apply rotation\n",
    "#     rotated_image = rotate_image(image)\n",
    "    \n",
    "#     # Apply zoom\n",
    "#     zoomed_image = zoom_image(rotated_image)\n",
    "    \n",
    "#     # Apply flip\n",
    "#     flipped_image = flip_image(zoomed_image)\n",
    "\n",
    "#     return flipped_image\n",
    "\n",
    "# # Perform image augmentation on the training data\n",
    "# X_train_augmented = [augment_image(image) for image in X_train]\n",
    "\n",
    "# # Convert the augmented data back to NumPy array\n",
    "# X_train_augmented = np.array(X_train_augmented)\n",
    "\n",
    "# # Concatenate the original and augmented data\n",
    "# X_train_combined = np.concatenate((X_train, X_train_augmented))\n",
    "# y_train_combined = np.concatenate((y_train, y_train.copy()))  \n",
    "\n",
    "# # Ensure X_train_combined and y_train_combined have the correct shapes\n",
    "# print(X_train_combined.shape)\n",
    "# print(y_train_combined.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train_combined \n",
    "# y_train = y_train_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.stack(X_train, axis=0)\n",
    "# Y_train = np.stack(y_train, axis=0)\n",
    "# X_test = np.stack(X_test, axis=0)\n",
    "# Y_test = np.stack(y_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_multiclass_model(n_classes, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Build a multi-class logistic regression model using Keras.\n",
    "\n",
    "    Args:\n",
    "    n_classes: Number of classes in the dataset\n",
    "    learning_rate: The desired learning rate for SGD.\n",
    "\n",
    "    Returns:\n",
    "    model: A tf.keras model (graph).\n",
    "    \"\"\"\n",
    "   \n",
    "    model = keras.Sequential([\n",
    "        data_augmentation,\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(units=n_classes, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "model = build_multiclass_model(4, 0.01)\n",
    "\n",
    "history = model.fit(\n",
    "  x = X_train,\n",
    "  y = y_train,\n",
    "  epochs=5,\n",
    "  batch_size=128,\n",
    "  validation_split=0.1,\n",
    "  verbose=1)\n",
    "\n",
    "history = pd.DataFrame(history.history)\n",
    "display(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = np.argmax(model.predict(X_val), axis=-1)\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix as a 2D array.\n",
    "confusion_matrix = tf.math.confusion_matrix(y_val, test_predictions)\n",
    "\n",
    "# Use a heatmap plot to display it.\n",
    "ax = sns.heatmap(confusion_matrix, annot=True, fmt='.3g', cmap='Blues',\n",
    "                 xticklabels=['Cyst','Normal','Stone','Tumor'], yticklabels=['Cyst','Normal','Stone','Tumor'], \n",
    "                 cbar=False)\n",
    "\n",
    "# Add axis labels.\n",
    "ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_classes,   #should we change the n_classes to 4? since we only have 4 classes\n",
    "                hidden_layer_sizes=[],\n",
    "                activation='relu',\n",
    "                optimizer='SGD',\n",
    "                learning_rate=0.01):\n",
    "    tf.keras.backend.clear_session()\n",
    "    np.random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "    model = tf.keras.models.Sequential()\n",
    "    # Flatten the input shape\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    # Add hidden layers\n",
    "    for layer_size in hidden_layer_sizes:\n",
    "        model.add(tf.keras.layers.Dense(layer_size, activation=activation))\n",
    "    #Add the last neural network layer\n",
    "    model.add(tf.keras.layers.Dense(units=n_classes, activation='softmax'))\n",
    "    if optimizer == 'SGD':\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    elif optimizer == 'Adam':\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    elif optimizer == 'RMSprop':\n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer. Please choose from ‘SGD’, ‘Adam’, or ‘RMSprop’.\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the shape\n",
    "#### I don't think we need these anymore since I reshaped before smote!\n",
    "# X_train = np.array([np.asarray(x).flatten() for x in X_train]).astype(np.float32)\n",
    "# y_train = np.array([np.asarray(y) for y in y_train]).astype(np.float32)\n",
    "# X_test = np.array([np.asarray(x).flatten() for x in X_train]).astype(np.float32)\n",
    "# y_test = np.array([np.asarray(y) for y in y_train]).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_model(n_classes=4, hidden_layer_sizes=[128], activation='relu', optimizer='SGD', learning_rate=0.01)\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1, verbose=1)\n",
    "test_loss, test_accuracy = model.evaluate(X_val, y_val, verbose=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "plt.plot(train_accuracy, label='train_accuracy')\n",
    "plt.plot(val_accuracy, label='validation accuracy')\n",
    "plt.xticks(range(5))\n",
    "plt.xlabel('Train epochs')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(n_classes=4, hidden_layer_sizes=[128], activation='relu', optimizer='RMSprop', learning_rate=0.01)\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1, verbose=1)\n",
    "test_loss, test_accuracy = model.evaluate(X_val, y_val, verbose=2)\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "plt.plot(train_accuracy, label='train_accuracy')\n",
    "plt.plot(val_accuracy, label='validation accuracy')\n",
    "plt.xticks(range(5))\n",
    "plt.xlabel('Train epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(n_classes=4, hidden_layer_sizes=[128], activation='relu', optimizer='Adam', learning_rate=0.01)\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1, verbose=1)\n",
    "test_loss, test_accuracy = model.evaluate(X_val, y_val, verbose=2)\n",
    "\n",
    "\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "plt.plot(train_accuracy, label='train_accuracy')\n",
    "plt.plot(val_accuracy, label='validation accuracy')\n",
    "plt.xticks(range(5))\n",
    "plt.xlabel('Train epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_cnn_model(input_shape, n_classes, optimizer='SGD', learning_rate=0.01):\n",
    " \n",
    "    np.random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Convolutional layers\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    if optimizer == 'SGD':\n",
    "        model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    elif optimizer == 'Adam':\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    elif optimizer == 'RMSprop':\n",
    "        model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer. Please choose from ‘SGD’, ‘Adam’, or ‘RMSprop’.\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3)  # Change this according to your image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shape = (32, 32, 3)  \n",
    "X_train_reshaped = np.reshape(X_train, (-1, 256, 256, 1))  \n",
    "X_val_reshaped = np.reshape(X_val, (-1, 256, 256, 1))  \n",
    "X_test_reshaped = np.reshape(X_test, (-1, 256, 256, 1)) \n",
    "# Normalize pixel values to range [0, 1]\n",
    "X_train_reshaped = X_train_reshaped / 255.0\n",
    "X_val_reshaped = X_val_reshaped / 255.0\n",
    "X_test_reshaped = X_test_reshaped / 255.0\n",
    "\n",
    "\n",
    "X_train_resized = np.array([tf.image.resize(image, (32, 32)) for image in X_train_reshaped])\n",
    "X_val_resized = np.array([tf.image.resize(image, (32, 32)) for image in X_val_reshaped])\n",
    "X_test_resized = np.array([tf.image.resize(image, (32, 32)) for image in X_test_reshaped])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rgb = np.repeat(X_train_resized[..., np.newaxis], 3, axis=-1)\n",
    "X_val_rgb = np.repeat(X_val_resized[..., np.newaxis], 3, axis=-1)\n",
    "X_test_rgb = np.repeat(X_test_resized[..., np.newaxis], 3, axis=-1)\n",
    "\n",
    "# Reshape to remove the extra dimension\n",
    "X_train_rgb = np.squeeze(X_train_rgb, axis=3)\n",
    "X_val_rgb = np.squeeze(X_val_rgb, axis=3)\n",
    "X_test_rgb = np.squeeze(X_test_rgb, axis=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN model for 5 epochs\n",
    "cnn_model = build_cnn_model(input_shape, n_classes=4, optimizer='SGD', learning_rate=0.01)\n",
    "history_cnn = cnn_model.fit(X_train_rgb, y_train, epochs=10, batch_size=64, validation_split=0.1, verbose=1)\n",
    "test_loss_cnn, test_accuracy_cnn = cnn_model.evaluate(X_val_rgb, y_val, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_cnn = history_cnn.history['accuracy']\n",
    "val_accuracy_cnn = history_cnn.history['val_accuracy']\n",
    "\n",
    "plt.plot(train_accuracy_cnn, label='train_accuracy_cnn')\n",
    "plt.plot(val_accuracy_cnn, label='validation accuracy_cnn')\n",
    "plt.xticks(range(5))\n",
    "plt.xlabel('Train epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = build_cnn_model(input_shape, n_classes=4, optimizer='Adam', learning_rate=0.01)\n",
    "history_cnn = cnn_model.fit(X_train_rgb, y_train, epochs=10, batch_size=64, validation_split=0.1, verbose=1)\n",
    "test_loss_cnn, test_accuracy_cnn = cnn_model.evaluate(X_val_rgb, y_val, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_cnn = history_cnn.history['accuracy']\n",
    "val_accuracy_cnn = history_cnn.history['val_accuracy']\n",
    "\n",
    "plt.plot(train_accuracy_cnn, label='train_accuracy_cnn')\n",
    "plt.plot(val_accuracy_cnn, label='validation accuracy_cnn')\n",
    "plt.xticks(range(5))\n",
    "plt.xlabel('Train epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = build_cnn_model(input_shape, n_classes=4, optimizer='RMSprop', learning_rate=0.01)\n",
    "history_cnn = cnn_model.fit(X_train_rgb, y_train, epochs=10, batch_size=64, validation_split=0.1, verbose=1)\n",
    "test_loss_cnn, test_accuracy_cnn = cnn_model.evaluate(X_val_rgb, y_val, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy_cnn = history_cnn.history['accuracy']\n",
    "val_accuracy_cnn = history_cnn.history['val_accuracy']\n",
    "\n",
    "plt.plot(train_accuracy_cnn, label='train_accuracy_cnn')\n",
    "plt.plot(val_accuracy_cnn, label='validation accuracy_cnn')\n",
    "plt.xticks(range(5))\n",
    "plt.xlabel('Train epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test CNN Model and Create ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cnn_model.predict(X_test_rgb)\n",
    "class_predictions = np.argmax(predictions, axis=-1)\n",
    "print(class_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "num_classes = np.max(y_test) + 1  # Assuming class indices start from 0\n",
    "y_test_onehot = label_binarize(y_test, classes=np.arange(num_classes))\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(4):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_onehot[:,i], predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot the ROC curve for each class\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(4):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label='ROC curve (area = {:.2f}) for class {}'.format(roc_auc[i], i))\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Multi-Class CNN')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "w207_final",
   "language": "python",
   "name": "w207_final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
